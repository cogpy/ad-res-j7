# Automated Testing Pipeline for Continuous Validation
# Validates the todo-to-issues and file-representations workflows continuously

name: Automated Testing Pipeline

on:
  # Run on all pushes to main for continuous validation
  push:
    branches: [ "main", "develop" ]
  
  # Run on all pull requests for validation before merge
  pull_request:
    branches: [ "main", "develop" ]
  
  # Run on schedule for continuous monitoring (daily at 2 AM UTC)
  schedule:
    - cron: '0 2 * * *'
  
  # Allow manual trigger for on-demand validation
  workflow_dispatch:
    inputs:
      verbose:
        description: 'Enable verbose test output'
        required: false
        default: 'false'
        type: boolean

jobs:
  validate-workflows:
    runs-on: ubuntu-latest
    timeout-minutes: 15
    
    permissions:
      contents: read       # Required to read repository files
      issues: write        # Required to create issues on test failures
      checks: write        # Required to create check runs
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0  # Fetch all history for better analysis

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '18'
          cache: 'npm'

      - name: Install dependencies
        run: npm install

      - name: Run workflow validation tests
        id: run_tests
        run: |
          echo "ðŸš€ Running automated testing pipeline..."
          echo "trigger=${{ github.event_name }}" >> $GITHUB_OUTPUT
          echo "verbose=${{ inputs.verbose || 'false' }}" >> $GITHUB_OUTPUT
          
          if [ "${{ inputs.verbose }}" == "true" ]; then
            npm run test:verbose || echo "test_failed=true" >> $GITHUB_OUTPUT
          else
            npm test || echo "test_failed=true" >> $GITHUB_OUTPUT
          fi
          
      - name: Check test results
        run: |
          if [ -f "tests/comprehensive-test-results.json" ]; then
            echo "=== Test Results Summary ===" >> $GITHUB_STEP_SUMMARY
            
            # Extract key metrics using jq
            total_tests=$(jq -r '.overall.total_tests' tests/comprehensive-test-results.json)
            passed_tests=$(jq -r '.overall.passed_tests' tests/comprehensive-test-results.json)
            failed_tests=$(jq -r '.overall.failed_tests' tests/comprehensive-test-results.json)
            success_rate=$(jq -r '.overall.success_rate' tests/comprehensive-test-results.json)
            
            echo "ðŸ“Š **Test Metrics:**" >> $GITHUB_STEP_SUMMARY
            echo "- Total Tests: $total_tests" >> $GITHUB_STEP_SUMMARY
            echo "- Passed: $passed_tests" >> $GITHUB_STEP_SUMMARY
            echo "- Failed: $failed_tests" >> $GITHUB_STEP_SUMMARY
            echo "- Success Rate: ${success_rate}%" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            
            # Check individual test suite results
            validation_passed=$(jq -r '.validation.passed' tests/comprehensive-test-results.json)
            validation_total=$(jq -r '.validation.total' tests/comprehensive-test-results.json)
            integration_passed=$(jq -r '.integration.passed' tests/comprehensive-test-results.json)
            integration_total=$(jq -r '.integration.total' tests/comprehensive-test-results.json)
            
            echo "ðŸ“‹ **Validation Tests:** $validation_passed/$validation_total passed" >> $GITHUB_STEP_SUMMARY
            echo "ðŸ§ª **Integration Tests:** $integration_passed/$integration_total passed" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            
            # Show any failures
            validation_errors=$(jq -r '.validation.errors | length' tests/comprehensive-test-results.json)
            integration_errors=$(jq -r '.integration.errors | length' tests/comprehensive-test-results.json)
            
            if [ "$validation_errors" -gt 0 ] || [ "$integration_errors" -gt 0 ]; then
              echo "âš ï¸ **Failed Tests:**" >> $GITHUB_STEP_SUMMARY
              
              if [ "$validation_errors" -gt 0 ]; then
                echo "Validation failures:" >> $GITHUB_STEP_SUMMARY
                jq -r '.validation.errors[]' tests/comprehensive-test-results.json | while read error; do
                  echo "- $error" >> $GITHUB_STEP_SUMMARY
                done
              fi
              
              if [ "$integration_errors" -gt 0 ]; then
                echo "Integration failures:" >> $GITHUB_STEP_SUMMARY
                jq -r '.integration.errors[]' tests/comprehensive-test-results.json | while read error; do
                  echo "- $error" >> $GITHUB_STEP_SUMMARY
                done
              fi
            else
              echo "ðŸŽ‰ **All tests passed!**" >> $GITHUB_STEP_SUMMARY
            fi
            
            # Determine overall status
            if [ "$success_rate" -ge 90 ]; then
              echo "âœ… Workflow validation successful (${success_rate}% pass rate)" >> $GITHUB_STEP_SUMMARY
            else
              echo "âŒ Workflow validation needs attention (${success_rate}% pass rate)" >> $GITHUB_STEP_SUMMARY
              exit 1
            fi
          else
            echo "âŒ Test results file not found" >> $GITHUB_STEP_SUMMARY
            exit 1
          fi

      - name: Upload test artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: workflow-test-results
          path: |
            tests/workflow-validation-results.json
            tests/integration-test-results.json
            tests/comprehensive-test-results.json
          retention-days: 30

      - name: Validate specific workflow requirements
        run: |
          echo "ðŸ” Checking specific workflow requirements..."
          
          # Check if todo-to-issues workflow exists and has required components
          if [ -f ".github/workflows/todo-to-issues.yml" ]; then
            echo "âœ… todo-to-issues.yml exists"
            
            # Check for label handling fix (from todo requirements)
            if grep -q "jq -r '\\.\\[\\]'" .github/workflows/todo-to-issues.yml; then
              echo "âœ… Label array handling implemented correctly"
            else
              echo "âŒ Label array handling needs review"
            fi
            
            # Check for comprehensive logging
            if grep -q "echo.*Creating issue:" .github/workflows/todo-to-issues.yml; then
              echo "âœ… Comprehensive logging implemented"
            else
              echo "âš ï¸ Consider adding more detailed logging"
            fi
          else
            echo "âŒ todo-to-issues.yml not found"
            exit 1
          fi
          
          # Check if file-representations workflow exists
          if [ -f ".github/workflows/file-representations.yml" ]; then
            echo "âœ… file-representations.yml exists"
          else
            echo "âŒ file-representations.yml not found"
            exit 1
          fi
          
          echo "ðŸŽ¯ Workflow requirement validation complete"
      
      - name: Create test status badge data
        if: github.ref == 'refs/heads/main'
        run: |
          # Create a simple badge data file for status tracking
          mkdir -p .github/badges
          
          if [ -f "tests/comprehensive-test-results.json" ]; then
            success_rate=$(jq -r '.overall.success_rate' tests/comprehensive-test-results.json)
            
            if [ "$success_rate" -ge 95 ]; then
              echo "passing" > .github/badges/test-status.txt
            elif [ "$success_rate" -ge 90 ]; then
              echo "mostly-passing" > .github/badges/test-status.txt
            else
              echo "failing" > .github/badges/test-status.txt
            fi
            
            echo "$success_rate" > .github/badges/test-rate.txt
            date -u +"%Y-%m-%d %H:%M:%S UTC" > .github/badges/test-date.txt
          fi
      
      - name: Report test failures on schedule
        if: failure() && github.event_name == 'schedule'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            
            // Read test results
            let testResults = {};
            try {
              testResults = JSON.parse(fs.readFileSync('tests/comprehensive-test-results.json', 'utf8'));
            } catch (error) {
              console.log('Could not read test results');
              return;
            }
            
            // Create issue for scheduled test failure
            const issueTitle = `Automated Test Pipeline Failure - ${new Date().toISOString().split('T')[0]}`;
            const issueBody = `## ðŸš¨ Scheduled Test Run Failed
            
            The automated testing pipeline detected failures during scheduled validation.
            
            ### Test Results
            - **Total Tests**: ${testResults.overall?.total_tests || 'N/A'}
            - **Passed**: ${testResults.overall?.passed_tests || 'N/A'}
            - **Failed**: ${testResults.overall?.failed_tests || 'N/A'}
            - **Success Rate**: ${testResults.overall?.success_rate || 'N/A'}%
            
            ### Failed Tests
            ${testResults.validation?.errors?.map(e => `- ${e}`).join('\n') || 'No validation errors'}
            ${testResults.integration?.errors?.map(e => `- ${e}`).join('\n') || 'No integration errors'}
            
            ### Action Required
            Please investigate these test failures and fix any issues.
            
            **Workflow Run**: ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}
            **Triggered**: Scheduled (daily validation)
            `;
            
            // Check if similar issue already exists
            const issues = await github.rest.issues.listForRepo({
              owner: context.repo.owner,
              repo: context.repo.repo,
              state: 'open',
              labels: ['automated-test-failure']
            });
            
            if (issues.data.length === 0) {
              await github.rest.issues.create({
                owner: context.repo.owner,
                repo: context.repo.repo,
                title: issueTitle,
                body: issueBody,
                labels: ['automated-test-failure', 'bug', 'priority: high']
              });
            }