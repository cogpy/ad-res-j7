# Automated Testing Pipeline for Continuous Validation
# Validates the todo-to-issues and file-representations workflows continuously

name: Automated Testing Pipeline

on:
  # Run on all pushes to main for continuous validation
  push:
    branches: [ "main", "develop" ]
  
  # Run on all pull requests for validation before merge
  pull_request:
    branches: [ "main", "develop" ]
  
  # Run on schedule for continuous monitoring (daily at 2 AM UTC)
  schedule:
    - cron: '0 2 * * *'
  
  # Allow manual trigger for on-demand validation
  workflow_dispatch:
    inputs:
      verbose:
        description: 'Enable verbose test output'
        required: false
        default: 'false'
        type: boolean

jobs:
  validate-workflows:
    runs-on: ubuntu-latest
    timeout-minutes: 15
    
    permissions:
      contents: read       # Required to read repository files
      issues: write        # Required to create issues on test failures
      checks: write        # Required to create check runs
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0  # Fetch all history for better analysis

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '18'
          cache: 'npm'

      - name: Install dependencies
        run: npm install

      - name: Run workflow validation tests
        id: run_tests
        run: |
          echo "ðŸš€ Running automated testing pipeline..."
          echo "trigger=${{ github.event_name }}" >> $GITHUB_OUTPUT
          echo "verbose=${{ inputs.verbose || 'false' }}" >> $GITHUB_OUTPUT
          
          if [ "${{ inputs.verbose }}" == "true" ]; then
            npm run test:verbose || echo "test_failed=true" >> $GITHUB_OUTPUT
          else
            npm test || echo "test_failed=true" >> $GITHUB_OUTPUT
          fi
          
      - name: Check test results
        run: |
          if [ -f "tests/comprehensive-test-results.json" ]; then
            echo "=== Test Results Summary ===" >> $GITHUB_STEP_SUMMARY
            
            # Extract key metrics using jq
            total_tests=$(jq -r '.overall.total_tests' tests/comprehensive-test-results.json)
            passed_tests=$(jq -r '.overall.passed_tests' tests/comprehensive-test-results.json)
            failed_tests=$(jq -r '.overall.failed_tests' tests/comprehensive-test-results.json)
            success_rate=$(jq -r '.overall.success_rate' tests/comprehensive-test-results.json)
            
            echo "ðŸ“Š **Test Metrics:**" >> $GITHUB_STEP_SUMMARY
            echo "- Total Tests: $total_tests" >> $GITHUB_STEP_SUMMARY
            echo "- Passed: $passed_tests" >> $GITHUB_STEP_SUMMARY
            echo "- Failed: $failed_tests" >> $GITHUB_STEP_SUMMARY
            echo "- Success Rate: ${success_rate}%" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            
            # Check individual test suite results
            validation_passed=$(jq -r '.validation.passed' tests/comprehensive-test-results.json)
            validation_total=$(jq -r '.validation.total' tests/comprehensive-test-results.json)
            integration_passed=$(jq -r '.integration.passed' tests/comprehensive-test-results.json)
            integration_total=$(jq -r '.integration.total' tests/comprehensive-test-results.json)
            
            echo "ðŸ“‹ **Validation Tests:** $validation_passed/$validation_total passed" >> $GITHUB_STEP_SUMMARY
            echo "ðŸ§ª **Integration Tests:** $integration_passed/$integration_total passed" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            
            # Show any failures
            validation_errors=$(jq -r '.validation.errors | length' tests/comprehensive-test-results.json)
            integration_errors=$(jq -r '.integration.errors | length' tests/comprehensive-test-results.json)
            
            if [ "$validation_errors" -gt 0 ] || [ "$integration_errors" -gt 0 ]; then
              echo "âš ï¸ **Failed Tests:**" >> $GITHUB_STEP_SUMMARY
              
              if [ "$validation_errors" -gt 0 ]; then
                echo "Validation failures:" >> $GITHUB_STEP_SUMMARY
                jq -r '.validation.errors[]' tests/comprehensive-test-results.json | while read error; do
                  echo "- $error" >> $GITHUB_STEP_SUMMARY
                done
              fi
              
              if [ "$integration_errors" -gt 0 ]; then
                echo "Integration failures:" >> $GITHUB_STEP_SUMMARY
                jq -r '.integration.errors[]' tests/comprehensive-test-results.json | while read error; do
                  echo "- $error" >> $GITHUB_STEP_SUMMARY
                done
              fi
            else
              echo "ðŸŽ‰ **All tests passed!**" >> $GITHUB_STEP_SUMMARY
            fi
            
            # Determine overall status
            if [ "$success_rate" -ge 90 ]; then
              echo "âœ… Workflow validation successful (${success_rate}% pass rate)" >> $GITHUB_STEP_SUMMARY
            else
              echo "âŒ Workflow validation needs attention (${success_rate}% pass rate)" >> $GITHUB_STEP_SUMMARY
              exit 1
            fi
          else
            echo "âŒ Test results file not found" >> $GITHUB_STEP_SUMMARY
            exit 1
          fi

      - name: Upload test artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: workflow-test-results
          path: |
            tests/workflow-validation-results.json
            tests/integration-test-results.json
            tests/comprehensive-test-results.json
          retention-days: 30

      - name: Validate specific workflow requirements
        run: |
          echo "ðŸ” Checking specific workflow requirements..."
          
          # Check if todo-to-issues workflow exists and has required components
          if [ -f ".github/workflows/todo-to-issues.yml" ]; then
            echo "âœ… todo-to-issues.yml exists"
            
            # Check for label handling fix (from todo requirements)
            if grep -q "jq -r '\\.\\[\\]'" .github/workflows/todo-to-issues.yml; then
              echo "âœ… Label array handling implemented correctly"
            else
              echo "âŒ Label array handling needs review"
            fi
            
            # Check for comprehensive logging
            if grep -q "echo.*Creating issue:" .github/workflows/todo-to-issues.yml; then
              echo "âœ… Comprehensive logging implemented"
            else
              echo "âš ï¸ Consider adding more detailed logging"
            fi
          else
            echo "âŒ todo-to-issues.yml not found"
            exit 1
          fi
          
          # Check if file-representations workflow exists
          if [ -f ".github/workflows/file-representations.yml" ]; then
            echo "âœ… file-representations.yml exists"
          else
            echo "âŒ file-representations.yml not found"
            exit 1
          fi
          
          echo "ðŸŽ¯ Workflow requirement validation complete"
      
      - name: Create test status badge data
        if: github.ref == 'refs/heads/main'
        run: |
          # Create a simple badge data file for status tracking
          mkdir -p .github/badges
          
          if [ -f "tests/comprehensive-test-results.json" ]; then
            success_rate=$(jq -r '.overall.success_rate' tests/comprehensive-test-results.json)
            
            if [ "$success_rate" -ge 95 ]; then
              echo "passing" > .github/badges/test-status.txt
            elif [ "$success_rate" -ge 90 ]; then
              echo "mostly-passing" > .github/badges/test-status.txt
            else
              echo "failing" > .github/badges/test-status.txt
            fi
            
            echo "$success_rate" > .github/badges/test-rate.txt
            date -u +"%Y-%m-%d %H:%M:%S UTC" > .github/badges/test-date.txt
          fi
      
      - name: Report test failures on schedule
        if: failure() && github.event_name == 'schedule'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            
            // Read test results
            let testResults = {};
            try {
              testResults = JSON.parse(fs.readFileSync('tests/comprehensive-test-results.json', 'utf8'));
            } catch (error) {
              console.log('Could not read test results');
              return;
            }
            
            // Create issue for scheduled test failure
            const issueTitle = `Automated Test Pipeline Failure - ${new Date().toISOString().split('T')[0]}`;
            const issueBody = `## ðŸš¨ Scheduled Test Run Failed
            
            The automated testing pipeline detected failures during scheduled validation.
            
            ### Test Results
            - **Total Tests**: ${testResults.overall?.total_tests || 'N/A'}
            - **Passed**: ${testResults.overall?.passed_tests || 'N/A'}
            - **Failed**: ${testResults.overall?.failed_tests || 'N/A'}
            - **Success Rate**: ${testResults.overall?.success_rate || 'N/A'}%
            
            ### Failed Tests
            ${testResults.validation?.errors?.map(e => `- ${e}`).join('\n') || 'No validation errors'}
            ${testResults.integration?.errors?.map(e => `- ${e}`).join('\n') || 'No integration errors'}
            
            ### Action Required
            Please investigate these test failures and fix any issues.
            
            **Workflow Run**: ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}
            **Triggered**: Scheduled (daily validation)
            `;
            
            // Check if similar issue already exists
            const issues = await github.rest.issues.listForRepo({
              owner: context.repo.owner,
              repo: context.repo.repo,
              state: 'open',
              labels: ['automated-test-failure']
            });
            
            if (issues.data.length === 0) {
              await github.rest.issues.create({
                owner: context.repo.owner,
                repo: context.repo.repo,
                title: issueTitle,
                body: issueBody,
                labels: ['automated-test-failure', 'bug', 'priority: high']
              });
            }
      
      - name: Monitor workflow failure patterns
        if: always()
        uses: actions/github-script@v7
        with:
          script: |
            // Collect workflow execution metrics and failure patterns
            const workflowMetrics = {
              runId: context.runId,
              workflow: 'test-workflows',
              status: '${{ job.status }}',
              trigger: context.eventName,
              branch: context.ref,
              commit: context.sha,
              actor: context.actor,
              timestamp: new Date().toISOString(),
              executionTime: process.env.GITHUB_ACTION_STARTED_AT ? 
                (Date.now() - new Date(process.env.GITHUB_ACTION_STARTED_AT).getTime()) / 1000 : null
            };
            
            console.log('Workflow Execution Metrics:', JSON.stringify(workflowMetrics, null, 2));
            
            // Check for failure patterns in recent workflow runs
            try {
              const recentRuns = await github.rest.actions.listWorkflowRuns({
                owner: context.repo.owner,
                repo: context.repo.repo,
                workflow_id: 'test-workflows.yml',
                per_page: 10,
                status: 'completed'
              });
              
              const failures = recentRuns.data.workflow_runs.filter(run => 
                run.conclusion === 'failure' || run.conclusion === 'cancelled'
              );
              
              const failureRate = failures.length / Math.min(recentRuns.data.workflow_runs.length, 10);
              
              console.log(`Recent failure analysis: ${failures.length}/${recentRuns.data.workflow_runs.length} runs failed (${(failureRate * 100).toFixed(1)}%)`);
              
              // Alert on high failure rate (>30% in last 10 runs)
              if (failureRate > 0.3 && failures.length >= 3) {
                const patternIssueTitle = `High Test Workflow Failure Rate Detected - ${new Date().toISOString().split('T')[0]}`;
                const patternIssueBody = `## ðŸš¨ Workflow Reliability Alert
                
                A high failure rate has been detected in the automated testing pipeline.
                
                ### Failure Pattern Analysis
                - **Failure Rate**: ${(failureRate * 100).toFixed(1)}% (${failures.length}/${recentRuns.data.workflow_runs.length} recent runs)
                - **Threshold**: 30% failure rate threshold exceeded
                - **Detection Time**: ${new Date().toISOString()}
                
                ### Recent Failed Runs
                ${failures.slice(0, 5).map(run => 
                  `- [Run ${run.run_number}](${run.html_url}) - ${run.conclusion} (${new Date(run.created_at).toLocaleDateString()})`
                ).join('\n')}
                
                ### Impact Assessment
                - âš ï¸ **Critical**: Test pipeline reliability is compromised
                - âš ï¸ **Quality**: Code quality validation may be inconsistent  
                - âš ï¸ **CI/CD**: Deployment confidence is reduced
                
                ### Recommended Actions
                1. ðŸ” **Investigate Pattern**: Review failed run logs for common issues
                2. ðŸ“Š **Analyze Trends**: Check if failures correlate with specific changes or times
                3. ðŸ”§ **Address Root Cause**: Fix underlying infrastructure or code issues
                4. ðŸ§ª **Validate Fixes**: Run multiple test cycles to confirm stability
                5. ðŸ“ˆ **Monitor Recovery**: Track failure rate improvement over next runs
                
                ### Workflow Health Monitoring
                This alert was generated automatically when the failure rate exceeded 30% over the last 10 runs.
                
                ---
                
                *Generated by workflow monitoring system - Pattern Detection Alert*
                `;
                
                // Check if pattern alert already exists
                const existingPatternIssues = await github.rest.issues.listForRepo({
                  owner: context.repo.owner,
                  repo: context.repo.repo,
                  state: 'open',
                  labels: ['workflow-reliability-alert'],
                  per_page: 3
                });
                
                if (existingPatternIssues.data.length === 0) {
                  await github.rest.issues.create({
                    owner: context.repo.owner,
                    repo: context.repo.repo,
                    title: patternIssueTitle,
                    body: patternIssueBody,
                    labels: ['workflow-reliability-alert', 'infrastructure', 'priority: critical']
                  });
                  
                  console.log('Created workflow reliability alert due to high failure rate');
                } else {
                  console.log('Pattern alert already exists, skipping duplicate creation');
                }
              }
              
            } catch (error) {
              console.log('Could not analyze workflow patterns:', error.message);
            }
      
      - name: Update workflow health dashboard
        if: github.ref == 'refs/heads/main'
        run: |
          # Create workflow health metrics for dashboard
          mkdir -p .github/workflow-health
          
          # Capture current run metrics
          cat > .github/workflow-health/latest-run.json << EOF
          {
            "timestamp": "$(date -u +"%Y-%m-%dT%H:%M:%SZ")",
            "runId": "${{ github.run_id }}",
            "status": "${{ job.status }}",
            "trigger": "${{ github.event_name }}",
            "branch": "${{ github.ref }}",
            "commit": "${{ github.sha }}",
            "actor": "${{ github.actor }}",
            "workflow": "test-workflows",
            "success_rate": $(if [ -f "tests/comprehensive-test-results.json" ]; then jq -r '.overall.success_rate // 0' tests/comprehensive-test-results.json; else echo 0; fi)
          }
          EOF
          
          # Update health status badge
          if [ "${{ job.status }}" = "success" ]; then
            echo "healthy" > .github/workflow-health/status.txt
          else
            echo "failing" > .github/workflow-health/status.txt
          fi
          
          echo "$(date -u +"%Y-%m-%d %H:%M:%S UTC")" > .github/workflow-health/last-check.txt
          
          # Log health update
          echo "ðŸ“Š Updated workflow health dashboard:"
          echo "  Status: $(cat .github/workflow-health/status.txt)"
          echo "  Last Check: $(cat .github/workflow-health/last-check.txt)"
          
          if [ -f "tests/comprehensive-test-results.json" ]; then
            success_rate=$(jq -r '.overall.success_rate' tests/comprehensive-test-results.json)
            echo "  Test Success Rate: ${success_rate}%"
          fi