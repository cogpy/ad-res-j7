# Automated Testing Pipeline for Continuous Validation
# Validates the todo-to-issues and file-representations workflows continuously

name: Automated Testing Pipeline

on:
  # Run on all pushes to main for continuous validation
  push:
    branches: [ "main", "develop" ]
  
  # Run on all pull requests for validation before merge
  pull_request:
    branches: [ "main", "develop" ]
  
  # Run on schedule for continuous monitoring (every 4 hours for enhanced coverage)
  schedule:
    - cron: '0 */4 * * *'  # Every 4 hours for continuous validation
    - cron: '0 2 * * *'     # Daily comprehensive analysis at 2 AM UTC
  
  # Allow manual trigger for on-demand validation
  workflow_dispatch:
    inputs:
      verbose:
        description: 'Enable verbose test output'
        required: false
        default: 'false'
        type: boolean
      test_suite:
        description: 'Test suite to run (all, validation, integration, security)'
        required: false
        default: 'all'
        type: choice
        options:
          - all
          - validation
          - integration
          - security
          - comprehensive

jobs:
  validate-workflows:
    runs-on: ubuntu-latest
    timeout-minutes: 15
    
    permissions:
      contents: read       # Required to read repository files
      issues: write        # Required to create issues on test failures
      checks: write        # Required to create check runs
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0  # Fetch all history for better analysis

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '18'
          cache: 'npm'

      - name: Install dependencies
        run: npm install

      - name: Run workflow validation tests
        id: run_tests
        run: |
          echo "üöÄ Running automated testing pipeline for continuous validation..."
          echo "trigger=${{ github.event_name }}" >> $GITHUB_OUTPUT
          echo "verbose=${{ inputs.verbose || 'false' }}" >> $GITHUB_OUTPUT
          echo "test_suite=${{ inputs.test_suite || 'all' }}" >> $GITHUB_OUTPUT
          echo "is_scheduled=${{ github.event_name == 'schedule' }}" >> $GITHUB_OUTPUT
          
          # Enhanced test execution with selective test suites for continuous validation
          test_suite="${{ inputs.test_suite || 'all' }}"
          verbose="${{ inputs.verbose || 'false' }}"
          
          case "$test_suite" in
            "validation")
              echo "üìã Running validation tests only..."
              npm run test:validation || echo "test_failed=true" >> $GITHUB_OUTPUT
              ;;
            "integration")
              echo "üß™ Running integration tests only..."
              npm run test:integration || echo "test_failed=true" >> $GITHUB_OUTPUT
              ;;
            "security")
              echo "üîí Running security tests only..."
              npm run test:security || echo "test_failed=true" >> $GITHUB_OUTPUT
              ;;
            "comprehensive")
              echo "üìä Running comprehensive tests only..."
              npm run test:comprehensive || echo "test_failed=true" >> $GITHUB_OUTPUT
              ;;
            *)
              echo "üéØ Running full test suite for continuous validation..."
              if [ "$verbose" == "true" ]; then
                npm run test:verbose || echo "test_failed=true" >> $GITHUB_OUTPUT
              else
                npm test || echo "test_failed=true" >> $GITHUB_OUTPUT
              fi
              ;;
          esac
          
      - name: Check test results and enforce quality gates
        run: |
          if [ -f "tests/comprehensive-test-results.json" ]; then
            echo "=== Continuous Validation Results ===" >> $GITHUB_STEP_SUMMARY
            
            # Extract key metrics using jq
            total_tests=$(jq -r '.overall.total_tests' tests/comprehensive-test-results.json)
            passed_tests=$(jq -r '.overall.passed_tests' tests/comprehensive-test-results.json)
            failed_tests=$(jq -r '.overall.failed_tests' tests/comprehensive-test-results.json)
            success_rate=$(jq -r '.overall.success_rate' tests/comprehensive-test-results.json)
            
            echo "üìä **Continuous Validation Metrics:**" >> $GITHUB_STEP_SUMMARY
            echo "- Total Tests: $total_tests" >> $GITHUB_STEP_SUMMARY
            echo "- Passed: $passed_tests" >> $GITHUB_STEP_SUMMARY
            echo "- Failed: $failed_tests" >> $GITHUB_STEP_SUMMARY
            echo "- Success Rate: ${success_rate}%" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            
            # Quality Gate Enforcement for Continuous Validation
            echo "üö™ **Quality Gates:**" >> $GITHUB_STEP_SUMMARY
            
            # Critical security tests must pass (100% for critical security issues)
            security_critical=$(jq -r '.security.critical_findings // 0' tests/comprehensive-test-results.json)
            if [ "$security_critical" -gt 0 ]; then
              echo "‚ùå **GATE FAILED**: Critical security findings detected ($security_critical)" >> $GITHUB_STEP_SUMMARY
              echo "::error::Critical security issues block continuous validation"
              exit 1
            else
              echo "‚úÖ **Security Gate**: No critical security findings" >> $GITHUB_STEP_SUMMARY
            fi
            
            # Overall success rate must be >= 85% for continuous validation
            if [ "$success_rate" -ge 85 ]; then
              echo "‚úÖ **Quality Gate**: Success rate meets continuous validation threshold (${success_rate}% >= 85%)" >> $GITHUB_STEP_SUMMARY
            else
              echo "‚ùå **GATE FAILED**: Success rate below continuous validation threshold (${success_rate}% < 85%)" >> $GITHUB_STEP_SUMMARY
              echo "::error::Test quality below acceptable threshold for continuous validation"
              exit 1
            fi
            
            # Check individual test suite results
            validation_passed=$(jq -r '.validation.passed' tests/comprehensive-test-results.json)
            validation_total=$(jq -r '.validation.total' tests/comprehensive-test-results.json)
            integration_passed=$(jq -r '.integration.passed' tests/comprehensive-test-results.json)
            integration_total=$(jq -r '.integration.total' tests/comprehensive-test-results.json)
            
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "üìã **Test Suite Results:**" >> $GITHUB_STEP_SUMMARY
            echo "- **Validation Tests:** $validation_passed/$validation_total passed" >> $GITHUB_STEP_SUMMARY
            echo "- **Integration Tests:** $integration_passed/$integration_total passed" >> $GITHUB_STEP_SUMMARY
            
            # Show any failures
            validation_errors=$(jq -r '.validation.errors | length' tests/comprehensive-test-results.json)
            integration_errors=$(jq -r '.integration.errors | length' tests/comprehensive-test-results.json)
            
            if [ "$validation_errors" -gt 0 ] || [ "$integration_errors" -gt 0 ]; then
              echo "" >> $GITHUB_STEP_SUMMARY
              echo "‚ö†Ô∏è **Failed Tests:**" >> $GITHUB_STEP_SUMMARY
              
              if [ "$validation_errors" -gt 0 ]; then
                echo "Validation failures:" >> $GITHUB_STEP_SUMMARY
                jq -r '.validation.errors[]' tests/comprehensive-test-results.json | while read error; do
                  echo "- $error" >> $GITHUB_STEP_SUMMARY
                done
              fi
              
              if [ "$integration_errors" -gt 0 ]; then
                echo "Integration failures:" >> $GITHUB_STEP_SUMMARY
                jq -r '.integration.errors[]' tests/comprehensive-test-results.json | while read error; do
                  echo "- $error" >> $GITHUB_STEP_SUMMARY
                done
              fi
            else
              echo "" >> $GITHUB_STEP_SUMMARY
              echo "üéâ **All quality gates passed for continuous validation!**" >> $GITHUB_STEP_SUMMARY
            fi
            
          else
            echo "‚ùå Test results file not found" >> $GITHUB_STEP_SUMMARY
            echo "::error::Continuous validation requires test results"
            exit 1
          fi

      # New step for continuous validation trend analysis
      - name: Analyze test trends for continuous validation
        if: github.event_name == 'schedule' || github.event_name == 'workflow_dispatch'
        run: |
          echo "üìà Analyzing test trends for continuous validation..."
          
          # Create test trend analysis
          mkdir -p .github/test-trends
          
          # Store current test results with timestamp
          timestamp=$(date -u +"%Y-%m-%d_%H-%M-%S")
          if [ -f "tests/comprehensive-test-results.json" ]; then
            # Extract key metrics for trend analysis
            success_rate=$(jq -r '.overall.success_rate' tests/comprehensive-test-results.json)
            total_tests=$(jq -r '.overall.total_tests' tests/comprehensive-test-results.json)
            failed_tests=$(jq -r '.overall.failed_tests' tests/comprehensive-test-results.json)
            security_findings=$(jq -r '.security.security_findings // 0' tests/comprehensive-test-results.json)
            
            # Create trend data point
            cat > .github/test-trends/trend-${timestamp}.json << EOF
          {
            "timestamp": "$(date -u +"%Y-%m-%dT%H:%M:%SZ")",
            "success_rate": $success_rate,
            "total_tests": $total_tests,
            "failed_tests": $failed_tests,
            "security_findings": $security_findings,
            "trigger": "${{ github.event_name }}",
            "branch": "${{ github.ref }}",
            "commit": "${{ github.sha }}"
          }
          EOF
            
            echo "üìä Current validation metrics recorded for trend analysis"
            echo "- Success Rate: ${success_rate}%"
            echo "- Total Tests: $total_tests"
            echo "- Failed Tests: $failed_tests"
            echo "- Security Findings: $security_findings"
            
            # Analyze recent trends if we have historical data
            if ls .github/test-trends/trend-*.json >/dev/null 2>&1; then
              # Get last 5 trend data points
              recent_files=$(ls -t .github/test-trends/trend-*.json | head -5)
              recent_success_rates=$(for file in $recent_files; do jq -r '.success_rate' "$file" 2>/dev/null || echo "0"; done)
              
              # Calculate trend direction
              rates_array=($recent_success_rates)
              if [ ${#rates_array[@]} -ge 2 ]; then
                current_rate=${rates_array[0]}
                previous_rate=${rates_array[1]}
                
                if [ "$current_rate" -gt "$previous_rate" ]; then
                  trend="üìà IMPROVING"
                elif [ "$current_rate" -lt "$previous_rate" ]; then
                  trend="üìâ DECLINING"
                else
                  trend="‚û°Ô∏è STABLE"
                fi
                
                echo "üîç **Trend Analysis:**" >> $GITHUB_STEP_SUMMARY
                echo "- **Current Success Rate**: ${current_rate}%" >> $GITHUB_STEP_SUMMARY
                echo "- **Previous Success Rate**: ${previous_rate}%" >> $GITHUB_STEP_SUMMARY
                echo "- **Trend**: $trend" >> $GITHUB_STEP_SUMMARY
                
                # Alert on declining trends
                if [ "$current_rate" -lt "$previous_rate" ] && [ "$current_rate" -lt 90 ]; then
                  echo "::warning::Test quality trend is declining - attention needed for continuous validation"
                fi
              fi
            fi
          fi

      - name: Upload test artifacts for continuous validation
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: continuous-validation-results-${{ github.run_id }}
          path: |
            tests/workflow-validation-results.json
            tests/integration-test-results.json
            tests/comprehensive-test-results.json
            .github/test-trends/
            .github/workflow-health/
          retention-days: 90  # Extended retention for trend analysis

      - name: Validate specific workflow requirements
        run: |
          echo "üîç Checking specific workflow requirements..."
          
          # Check if todo-to-issues workflow exists and has required components
          if [ -f ".github/workflows/todo-to-issues.yml" ]; then
            echo "‚úÖ todo-to-issues.yml exists"
            
            # Check for label handling fix (from todo requirements)
            if grep -q "jq -r '\\.\\[\\]'" .github/workflows/todo-to-issues.yml; then
              echo "‚úÖ Label array handling implemented correctly"
            else
              echo "‚ùå Label array handling needs review"
            fi
            
            # Check for comprehensive logging
            if grep -q "echo.*Creating issue:" .github/workflows/todo-to-issues.yml; then
              echo "‚úÖ Comprehensive logging implemented"
            else
              echo "‚ö†Ô∏è Consider adding more detailed logging"
            fi
          else
            echo "‚ùå todo-to-issues.yml not found"
            exit 1
          fi
          
          # Check if file-representations workflow exists
          if [ -f ".github/workflows/file-representations.yml" ]; then
            echo "‚úÖ file-representations.yml exists"
          else
            echo "‚ùå file-representations.yml not found"
            exit 1
          fi
          
          echo "üéØ Workflow requirement validation complete"
      
      - name: Create test status badge data
        if: github.ref == 'refs/heads/main'
        run: |
          # Create a simple badge data file for status tracking
          mkdir -p .github/badges
          
          if [ -f "tests/comprehensive-test-results.json" ]; then
            success_rate=$(jq -r '.overall.success_rate' tests/comprehensive-test-results.json)
            
            if [ "$success_rate" -ge 95 ]; then
              echo "passing" > .github/badges/test-status.txt
            elif [ "$success_rate" -ge 90 ]; then
              echo "mostly-passing" > .github/badges/test-status.txt
            else
              echo "failing" > .github/badges/test-status.txt
            fi
            
            echo "$success_rate" > .github/badges/test-rate.txt
            date -u +"%Y-%m-%d %H:%M:%S UTC" > .github/badges/test-date.txt
          fi
      
      - name: Report test failures on schedule
        if: failure() && github.event_name == 'schedule'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            
            // Read test results
            let testResults = {};
            try {
              testResults = JSON.parse(fs.readFileSync('tests/comprehensive-test-results.json', 'utf8'));
            } catch (error) {
              console.log('Could not read test results');
              return;
            }
            
            // Create issue for scheduled test failure
            const issueTitle = `Automated Test Pipeline Failure - ${new Date().toISOString().split('T')[0]}`;
            const issueBody = `## üö® Scheduled Test Run Failed
            
            The automated testing pipeline detected failures during scheduled validation.
            
            ### Test Results
            - **Total Tests**: ${testResults.overall?.total_tests || 'N/A'}
            - **Passed**: ${testResults.overall?.passed_tests || 'N/A'}
            - **Failed**: ${testResults.overall?.failed_tests || 'N/A'}
            - **Success Rate**: ${testResults.overall?.success_rate || 'N/A'}%
            
            ### Failed Tests
            ${testResults.validation?.errors?.map(e => `- ${e}`).join('\n') || 'No validation errors'}
            ${testResults.integration?.errors?.map(e => `- ${e}`).join('\n') || 'No integration errors'}
            
            ### Action Required
            Please investigate these test failures and fix any issues.
            
            **Workflow Run**: ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}
            **Triggered**: Scheduled (daily validation)
            `;
            
            // Check if similar issue already exists
            const issues = await github.rest.issues.listForRepo({
              owner: context.repo.owner,
              repo: context.repo.repo,
              state: 'open',
              labels: ['automated-test-failure']
            });
            
            if (issues.data.length === 0) {
              await github.rest.issues.create({
                owner: context.repo.owner,
                repo: context.repo.repo,
                title: issueTitle,
                body: issueBody,
                labels: ['automated-test-failure', 'bug', 'priority: high']
              });
            }
      
      - name: Monitor workflow failure patterns
        if: always()
        uses: actions/github-script@v7
        with:
          script: |
            // Collect workflow execution metrics and failure patterns
            const workflowMetrics = {
              runId: context.runId,
              workflow: 'test-workflows',
              status: '${{ job.status }}',
              trigger: context.eventName,
              branch: context.ref,
              commit: context.sha,
              actor: context.actor,
              timestamp: new Date().toISOString(),
              executionTime: process.env.GITHUB_ACTION_STARTED_AT ? 
                (Date.now() - new Date(process.env.GITHUB_ACTION_STARTED_AT).getTime()) / 1000 : null
            };
            
            console.log('Workflow Execution Metrics:', JSON.stringify(workflowMetrics, null, 2));
            
            // Check for failure patterns in recent workflow runs
            try {
              const recentRuns = await github.rest.actions.listWorkflowRuns({
                owner: context.repo.owner,
                repo: context.repo.repo,
                workflow_id: 'test-workflows.yml',
                per_page: 10,
                status: 'completed'
              });
              
              const failures = recentRuns.data.workflow_runs.filter(run => 
                run.conclusion === 'failure' || run.conclusion === 'cancelled'
              );
              
              const failureRate = failures.length / Math.min(recentRuns.data.workflow_runs.length, 10);
              
              console.log(`Recent failure analysis: ${failures.length}/${recentRuns.data.workflow_runs.length} runs failed (${(failureRate * 100).toFixed(1)}%)`);
              
              // Alert on high failure rate (>30% in last 10 runs)
              if (failureRate > 0.3 && failures.length >= 3) {
                const patternIssueTitle = `High Test Workflow Failure Rate Detected - ${new Date().toISOString().split('T')[0]}`;
                const patternIssueBody = `## üö® Workflow Reliability Alert
                
                A high failure rate has been detected in the automated testing pipeline.
                
                ### Failure Pattern Analysis
                - **Failure Rate**: ${(failureRate * 100).toFixed(1)}% (${failures.length}/${recentRuns.data.workflow_runs.length} recent runs)
                - **Threshold**: 30% failure rate threshold exceeded
                - **Detection Time**: ${new Date().toISOString()}
                
                ### Recent Failed Runs
                ${failures.slice(0, 5).map(run => 
                  `- [Run ${run.run_number}](${run.html_url}) - ${run.conclusion} (${new Date(run.created_at).toLocaleDateString()})`
                ).join('\n')}
                
                ### Impact Assessment
                - ‚ö†Ô∏è **Critical**: Test pipeline reliability is compromised
                - ‚ö†Ô∏è **Quality**: Code quality validation may be inconsistent  
                - ‚ö†Ô∏è **CI/CD**: Deployment confidence is reduced
                
                ### Recommended Actions
                1. üîç **Investigate Pattern**: Review failed run logs for common issues
                2. üìä **Analyze Trends**: Check if failures correlate with specific changes or times
                3. üîß **Address Root Cause**: Fix underlying infrastructure or code issues
                4. üß™ **Validate Fixes**: Run multiple test cycles to confirm stability
                5. üìà **Monitor Recovery**: Track failure rate improvement over next runs
                
                ### Workflow Health Monitoring
                This alert was generated automatically when the failure rate exceeded 30% over the last 10 runs.
                
                ---
                
                *Generated by workflow monitoring system - Pattern Detection Alert*
                `;
                
                // Check if pattern alert already exists
                const existingPatternIssues = await github.rest.issues.listForRepo({
                  owner: context.repo.owner,
                  repo: context.repo.repo,
                  state: 'open',
                  labels: ['workflow-reliability-alert'],
                  per_page: 3
                });
                
                if (existingPatternIssues.data.length === 0) {
                  await github.rest.issues.create({
                    owner: context.repo.owner,
                    repo: context.repo.repo,
                    title: patternIssueTitle,
                    body: patternIssueBody,
                    labels: ['workflow-reliability-alert', 'infrastructure', 'priority: critical']
                  });
                  
                  console.log('Created workflow reliability alert due to high failure rate');
                } else {
                  console.log('Pattern alert already exists, skipping duplicate creation');
                }
              }
              
            } catch (error) {
              console.log('Could not analyze workflow patterns:', error.message);
            }
      
      - name: Update workflow health dashboard
        if: github.ref == 'refs/heads/main'
        run: |
          # Create workflow health metrics for dashboard
          mkdir -p .github/workflow-health
          
          # Capture current run metrics
          cat > .github/workflow-health/latest-run.json << EOF
          {
            "timestamp": "$(date -u +"%Y-%m-%dT%H:%M:%SZ")",
            "runId": "${{ github.run_id }}",
            "status": "${{ job.status }}",
            "trigger": "${{ github.event_name }}",
            "branch": "${{ github.ref }}",
            "commit": "${{ github.sha }}",
            "actor": "${{ github.actor }}",
            "workflow": "test-workflows",
            "success_rate": $(if [ -f "tests/comprehensive-test-results.json" ]; then jq -r '.overall.success_rate // 0' tests/comprehensive-test-results.json; else echo 0; fi)
          }
          EOF
          
          # Update health status badge
          if [ "${{ job.status }}" = "success" ]; then
            echo "healthy" > .github/workflow-health/status.txt
          else
            echo "failing" > .github/workflow-health/status.txt
          fi
          
          echo "$(date -u +"%Y-%m-%d %H:%M:%S UTC")" > .github/workflow-health/last-check.txt
          
          # Log health update
          echo "üìä Updated workflow health dashboard:"
          echo "  Status: $(cat .github/workflow-health/status.txt)"
          echo "  Last Check: $(cat .github/workflow-health/last-check.txt)"
          
          if [ -f "tests/comprehensive-test-results.json" ]; then
            success_rate=$(jq -r '.overall.success_rate' tests/comprehensive-test-results.json)
            echo "  Test Success Rate: ${success_rate}%"
          fi

  # New job for continuous validation monitoring
  continuous-validation-monitor:
    runs-on: ubuntu-latest
    needs: validate-workflows
    if: always() && github.event_name == 'schedule'
    timeout-minutes: 10
    
    permissions:
      contents: read
      issues: write
      actions: read
      
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        
      - name: Monitor continuous validation health
        uses: actions/github-script@v7
        with:
          script: |
            // Monitor the continuous validation pipeline health
            const fs = require('fs');
            
            console.log('üîç Monitoring continuous validation pipeline health...');
            
            // Get recent workflow runs for this workflow
            const recentRuns = await github.rest.actions.listWorkflowRuns({
              owner: context.repo.owner,
              repo: context.repo.repo,
              workflow_id: 'test-workflows.yml',
              per_page: 20,
              status: 'completed'
            });
            
            // Analyze failure patterns for continuous validation
            const failures = recentRuns.data.workflow_runs.filter(run => 
              run.conclusion === 'failure' || run.conclusion === 'cancelled'
            );
            
            const successCount = recentRuns.data.workflow_runs.filter(run => 
              run.conclusion === 'success'
            ).length;
            
            const totalRuns = Math.min(recentRuns.data.workflow_runs.length, 20);
            const failureRate = failures.length / totalRuns;
            const successRate = (successCount / totalRuns) * 100;
            
            console.log(`üìä Continuous Validation Health Metrics:`);
            console.log(`- Total Recent Runs: ${totalRuns}`);
            console.log(`- Successful Runs: ${successCount}`);
            console.log(`- Failed Runs: ${failures.length}`);
            console.log(`- Success Rate: ${successRate.toFixed(1)}%`);
            console.log(`- Failure Rate: ${(failureRate * 100).toFixed(1)}%`);
            
            // Critical health check: Alert if success rate drops below 70%
            if (successRate < 70) {
              const healthIssueTitle = `üö® Continuous Validation Health Alert - Success Rate Critical`;
              const healthIssueBody = `## üö® Critical: Continuous Validation Pipeline Health Degraded
              
              The automated testing pipeline for continuous validation has experienced significant reliability issues.
              
              ### Health Metrics
              - **Success Rate**: ${successRate.toFixed(1)}% (Critical: <70%)
              - **Total Recent Runs**: ${totalRuns}
              - **Failed Runs**: ${failures.length}
              - **Detection Time**: ${new Date().toISOString()}
              
              ### Impact Assessment
              - üî¥ **CRITICAL**: Continuous validation reliability severely compromised
              - üî¥ **QUALITY**: Code quality validation is unreliable
              - üî¥ **CI/CD**: Development workflow is significantly impacted
              - üî¥ **TRUST**: Team confidence in automation is eroded
              
              ### Recent Failed Runs
              ${failures.slice(0, 5).map(run => 
                \`- [Run \${run.run_number}](\${run.html_url}) - \${run.conclusion} (\${new Date(run.created_at).toLocaleDateString()})\`
              ).join('\\n')}
              
              ### Immediate Actions Required
              1. üö® **URGENT**: Investigate root cause of pipeline failures
              2. üîß **FIX**: Address infrastructure or configuration issues
              3. üß™ **VALIDATE**: Run manual validation to confirm fixes
              4. üìä **MONITOR**: Track recovery over next several runs
              5. üìù **DOCUMENT**: Record lessons learned and prevention measures
              
              ### Escalation
              This issue requires immediate engineering attention to restore continuous validation reliability.
              
              ---
              
              *Auto-generated by continuous validation health monitoring - CRITICAL ALERT*
              `;
              
              // Check for existing critical health issues
              const existingHealthIssues = await github.rest.issues.listForRepo({
                owner: context.repo.owner,
                repo: context.repo.repo,
                state: 'open',
                labels: ['continuous-validation-health', 'critical'],
                per_page: 3
              });
              
              if (existingHealthIssues.data.length === 0) {
                await github.rest.issues.create({
                  owner: context.repo.owner,
                  repo: context.repo.repo,
                  title: healthIssueTitle,
                  body: healthIssueBody,
                  labels: ['continuous-validation-health', 'critical', 'bug', 'priority: critical']
                });
                
                console.log('üö® Created critical health alert due to low success rate');
              } else {
                console.log('Critical health alert already exists, monitoring continues...');
              }
            }
            
            // Create health report for scheduled runs
            const healthReport = {
              timestamp: new Date().toISOString(),
              success_rate: successRate,
              failure_rate: failureRate * 100,
              total_runs: totalRuns,
              failed_runs: failures.length,
              health_status: successRate >= 90 ? 'healthy' : 
                           successRate >= 70 ? 'degraded' : 'critical',
              recent_failures: failures.slice(0, 3).map(run => ({
                run_number: run.run_number,
                conclusion: run.conclusion,
                created_at: run.created_at,
                html_url: run.html_url
              }))
            };
            
            console.log('üìä Health report generated for continuous validation monitoring');
            console.log(JSON.stringify(healthReport, null, 2));